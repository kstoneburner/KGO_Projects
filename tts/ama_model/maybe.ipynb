{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bec3ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/stonk013/KGO_Projects/tts/ama_model/training\n",
      "C:/Users/stonk013/KGO_Projects/tts/ama_model/tts_train_dir\n",
      "ama-01.wav 48000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# BaseDatasetConfig: defines name, formatter and path of the dataset.\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "\n",
    "#//*** create model output path if needed\n",
    "output_path = \"tts_train_dir\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "output_path = \"tts_train_dir\"\n",
    "training_dir = \"training\"\n",
    "output_path = os.path.join(os.path.join(os.getcwd(), output_path))\n",
    "output_path\n",
    "training_dir = os.path.join(os.path.join(os.getcwd(), training_dir)).replace(\"\\\\\",\"/\")\n",
    "print(str(training_dir))\n",
    "t_path = str(output_path).replace(\"\\\\\",\"/\")\n",
    "t_path\n",
    "print(t_path)\n",
    "\n",
    "import os\n",
    "import wave\n",
    "fulldir = os.path.join(output_path,\"wavs\")\n",
    "#//**** Get the sample rate of the first WAV file, assume all files share the same sample_rate\n",
    "for file_name in os.listdir(fulldir):\n",
    "    with wave.open(os.path.join(fulldir,file_name), \"rb\") as wave_file:\n",
    "        frame_rate = wave_file.getframerate()\n",
    "        print(file_name,frame_rate)\n",
    "        break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5963789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " > Training Environment:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Setting up Audio Processor...\n",
      " | > sample_rate:48000\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60.0\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " | > Found 12 files in C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " | > Backend: Torch\n",
      " | > Mixed precision: False\n",
      " | > Precision: float32\n",
      " | > Num. of CPUs: 8\n",
      " | > Num. of Torch Threads: 4\n",
      " | > Torch seed: 54321\n",
      " | > Torch CUDNN: True\n",
      " | > Torch CUDNN deterministic: False\n",
      " | > Torch CUDNN benchmark: False\n",
      " | > Torch TF32 MatMul: False\n",
      " > Start Tensorboard: tensorboard --logdir=C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\run-August-10-2023_09+29PM-0000000\n",
      "\n",
      " > Model has 28259417 parameters\n",
      "\n",
      "\u001b[4m\u001b[1m > EPOCH: 0/1000\u001b[0m\n",
      " --> C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\run-August-10-2023_09+29PM-0000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Pre-computing phonemes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 1/12 [00:00<00:08,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ænd tədeɪ kæl steɪt izi beɪ pɹəfɛsɚ əv kəmjunɪkeɪʃən ænd pəlɪtɪkəl ɛkspɚt noʊlən hɪɡdən spoʊk wɪθ ʌs lɪv əbaʊt haʊ ðɪs leɪtɪst ɪndaɪtmənt kʊd əfɛkt tɹʌmps t͡ʃænsɪz ɪn tu θaʊzənd twɛnti fɔɹ\n",
      "ænd tədeɪ kæl steɪt izi beɪ pɹəfɛsɚ əv kəmjunɪkeɪʃən ænd pəlɪtɪkəl ɛkspɚt noʊlən hɪɡdən spoʊk wɪθ ʌs lɪv əbaʊt haʊ ðɪs leɪtɪst ɪndaɪtmənt kʊd əfɛkt tɹʌmps t͡ʃænsɪz ɪn tu θaʊzənd twɛnti fɔɹ\n",
      " [!] Character '͡' not found in the vocabulary. Discarding it.\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 2/12 [00:01<00:05,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: æz njuz əv ðə ɪndaɪtmənt bɹoʊk ðɪs æftɚnun ju kʊd wɔt͡ʃ ɪt lɪv wi juz ðə əbk sɛvən njuz æp tə sɛnd aʊt ðɪs noʊtɪfɪkeɪʃən tə lɪv\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 3/12 [00:01<00:06,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ɪnvɛstəɡeɪtɚz ɑɹ wɚkɪŋ tə dɪtɚmən ðə kɔz əv ə fɔɹ əlɑɹm faɪɚ ɪn sæn fɹænsɪskoʊz heɪz væli ðə faɪɚ spɑɹkt ɚaʊnd sɪks ðɪs mɔɹnɪŋ ɔn oʊk ænd ɑkteɪviə stɹits fɔɹ nɪɹbaɪ bɪldɪŋz wɚ dæmɪd͡ʒd ðə hit kɹækt neɪbɚɪŋ wɪndoʊz ðə bɪldɪŋ səpoʊzd tə bi ə mɪkst jus mɑɹkɪt ɹeɪt haʊzɪŋ pɹɑd͡ʒɛkt wəz ʌndɚ kənstɹʌkʃən\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 4/12 [00:02<00:04,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: neɪbɚz toʊld ʌs ðeɪ si hoʊmləs pipəl ɡoʊ ɪnsaɪd æz stɑɹt faɪɚz bɪfɔɹ tu mʌnθs əɡoʊ ðə pɹɛzɪdənt əv ðə heɪz væli neɪbɚhʊd əsoʊʃieɪʃən ɹoʊt ə lɛtɚ tə ðə sɪti əbaʊt ðə nʌmbɚ əv faɪɚz sɚaʊndɪŋ tɛnt kæmps ɔn ɑkteɪviə stɹit\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|████▏     | 5/12 [00:02<00:03,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ɪn sæn fɹænsɪskoʊ fæmɪliz lɪvɪŋ ɪn ɹvz pɑɹkt kloʊs tə leɪk mɚst wɪl sun feɪs pɑɹkɪŋ ɹistɹɪkʃənz\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 6/12 [00:04<00:05,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: si vi ɛs plænz tə leɪɔf faɪv θaʊzənd ɪmplɔɪiz ðə dɹʌɡstɔɹ t͡ʃeɪn sɛz ɪts tɹaɪɪŋ tə kʌt kɔsts ðə leɪɔfs wɪl nɑt əfɛkt kʌstəmɚ feɪsɪŋ ɪmplɔɪiz ðeɪ wɪl moʊstli əfɛkt pipəl ɪn kɔɹpɚɪt ɹoʊlz leɪd ɔf wɚkɚz wɪl ɹɪsiv sɛvɚəns peɪ ænd bɛnəfɪts ænd sɚvɪsɪz tə faɪnd ənʌðɚ d͡ʒɑb ɹiɡəl sɪnəməz ðə wɚldz sɛkənd lɑɹd͡ʒɪst θiətɚ t͡ʃeɪn ɪz aʊt əv bæŋkɹəpsi ɪts pɛɹənt kʌmpəni saɪnwɚld sɛz ɪt hæz kʌt fɔɹ ænd ə hæf bɪljən dɑlɚz əv dɛt ɹeɪzd eɪt hʌndɹɪd mɪljən ɪn nju ɛkwɪti kæpɪtəl ænd sɪkjʊɹd fɪnænsɪŋ fɚ wʌn pɔɪnt sɛvən wʌn bɪljən dɑlɚz ðə t͡ʃeɪn faɪld fɚ bæŋkɹəpsi læst sɛptɛmbɚ ænd kloʊzd dʌzənz əv θiətɚz ɪŋkludɪŋ wʌn ɪn bɚkəli ðə pændɛmɪk wəz tʌf ɔn muvi θiətɚz bʌt blɑkbʌstɚ hɪts ðɪs sʌmɚ laɪk bɑɹbi ænd ɑpənhaɪmɚ hɛlpt tɚn θɪŋz ɚaʊnd\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|█████▊    | 7/12 [00:06<00:05,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ðə næzdæk wɛnt daʊn sɪksti tu ðə ɛs ænd pi lɔst twɛlv pɔɪnts bʌt ðæt ɪndɛks ɪz ʌp twɛnti wʌn pɚsɛnt soʊ fɑɹ ðɪs jɪɹ ðæts ðə lɑɹd͡ʒɪst ɡeɪn ɪn d͡ʒəlaɪ ɪn ə kwɔɹtɚ əv ə sɛnt͡ʃɚi hɛlp ɪz əveɪləbəl ɹaɪt naʊ fɚ pipəl ɪn ðə beɪ ɛɹiə hu ɑɹ oʊvɚdu peɪɪŋ ðɛɹ toʊlz ɪts θɹu fæstɹæk wɪt͡ʃ ɪz ðə sɪstəm ðæt t͡ʃɑɹd͡ʒɪz dɹaɪvɚz hu kɹɔs ðə toʊl bɹɪd͡ʒ ɔɹ juz ɛni əv ðə ɪkspɹɛs leɪnz ænd ɹaɪt naʊ ju ɑɹ lʊkɪŋ laɪv æt sʌm əv aʊɚ bɹɪd͡ʒɪz ðə ɹɪt͡ʃmənd sæn ɹɑfaɪɛl bɹɪd͡ʒ beɪ bɹɪd͡ʒ sæn mɑteɪoʊ bɹɪd͡ʒ θɹ bɹɪd͡ʒ ɔn ðə ɹaɪt hæz ðə moʊst tɹæfɪk naʊ ðɪs pɹoʊɡɹæm ɪn pɚtɪkjəlɚ ɪz ɡɪɹd təwɔɹd loʊ ɪŋkʌm fæmɪliz. pipəl kən faɪnd aʊt ɪf ðeɪ kwɑləfaɪ baɪ kɔlɪŋ fæstɹæk sɚvɪs ænd ðɛn əplaɪɪŋ ɔnlaɪn fɚ əsɪstəns hɪɹz haʊ ɪt wɚks.\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 8/12 [00:06<00:03,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ðæt wəz dɪskʌst baɪ kɑŋɡɹəsmən ɹoʊ kɑnə æt wʌn əv ðə wɚldz lɑɹd͡ʒɪst sɛmaɪ kəndʌktɚ pɹədusɚz ɪn sʌniveɪl tədeɪ eɪ bi si sɛvən njuz saʊθ beɪ ɹɪpɔɹtɚ zæk fwɛnteɪz wəz ðɛɹ\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 9/12 [00:07<00:02,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ə fɛdɚəl ɪnvɛstəɡeɪʃən ɪntu tɛsləz paʊɚ stɪɹɪŋ ɪz naʊ ʌndɚweɪ æftɚ dʌzənz əv kəmpleɪnts fɹʌm dɹaɪvɚz tu hʌndɹɪd iaɪti θaʊzənd viɪkəlz ɑɹ kʌvɚd baɪ ðə ɪnvɛstəɡeɪʃən kɑɹ oʊnɚz kəmpleɪn əv luzɪŋ paʊɚ stɪɹɪŋ ænd stɪɹɪŋ kəntɹoʊl ɪn tu θaʊzənd twɛnti θɹi tɛslə mɑdəl θɹi sɪdænz ænd mɑdəl waɪ sʌvz ðə næʃnəl haɪweɪ tɹæfɪk seɪfti ædmɪnɪstɹeɪʃən sɛz ðɛɹ ɪz wʌn ɹɪpɔɹt əv ə kɹæʃ bʌt noʊ ɪnd͡ʒɚiz fɹʌm ðɪs ɪʃu. fɛdɚəl seɪfti ɹɛɡjəleɪtɚz plæn tə lʊk ɪntu mænjəfækt͡ʃɚɪŋ sɪvɛɹɪti ænd haʊ ðə ɪʃu əkɚz ɪn ðə ɪnvɛstəɡeɪʃən\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n",
      "Text: \n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████▏| 11/12 [00:07<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: fɪl sɛz ðə pɹɑbləm ɪz nɑt junik tə oʊklənd ænd sæn fɹænsɪskoʊ ivən smɔlɚ sɪtiz laɪk sæn ɹəmoʊn ɑɹ ɪkspɪɹiənsɪŋ haɪ veɪkənsi ɹeɪts fɚ kəmɚʃəl ɹil ɪsteɪt\n",
      "cache_path C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\phoneme_cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:09<00:00,  1.21it/s]\n",
      "\n",
      "\u001b[1m > TRAINING (2023-08-10 21:29:47) \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: æktɚ zɛndeɪə tədeɪ poʊstɪd ə mɛsɪd͡ʒ ɹɪmɛmbɚɪŋ hɚ koʊstɑɹ ænd fɛloʊ oʊklənd neɪtɪv æŋɡəs klaʊd klaʊdz fæmɪli ənaʊnst hɪz dɛθ jɛstɚdeɪ hɪz fæmɪli sɛd hi stɹʌɡəld wɪθ ðə dɛθ əv hɪz fɑðɚ wɪt͡ʃ hæpənd d͡ʒʌst læst wik ænd sʌfɚd fɹʌm mɛntəl hɛlθ ɪʃuz zɛndeɪə ænd klaʊd koʊ stɑɹd ɪn ðə eɪt͡ʃ bi oʊ dɹɑmə sɪɹiz jufɔɹiə ænd boʊθ ətɛndɪd oʊklənd skul fɚ ðə ɑɹts zɛndeɪ ɹoʊt ɔn ɪnstəɡɹæm kwoʊt aɪ noʊ pipəl juz ðɪs ɪkspɹɛʃən ɔftən wɛn tɔkɪŋ əbaʊt foʊks ðeɪ lʌv ðeɪ kʊd laɪt ʌp ɛni ɹum ðeɪ ɛntɚd bʌt bɔɪ lɛt mi tɛl ju hi wəz ðə bɛst æt ɪt aɪd laɪk tə ɹɪmɛmbɚ hɪm ðæt weɪ klaʊd wəz twɛnti faɪv jɪɹz oʊld wi hæv pʊt təɡɛðɚ ə hoʊl lɪst əv ɹisɔɹsɪz fɚ ɛniwʌn dilɪŋ wɪθ lɔs, ɡɹif ɔɹ mɛntəl hɛlθ ɪʃuz ju kən faɪnd loʊkəl ɹisɔɹsɪz baɪ ɡoʊɪŋ tə əbksɛvən njuz dɑt kɑm slæʃ teɪk ækʃən ænd ɪf ju nid ɪt ðə suɪsaɪd ænd kɹaɪsəs laɪflaɪn kən bi ɹit͡ʃt baɪ daɪlɪŋ naɪn eɪt eɪt\n",
      "\n",
      "\n",
      "> DataLoader initialization\n",
      "| > Tokenizer:\n",
      "\t| > add_blank: False\n",
      "\t| > use_eos_bos: False\n",
      "\t| > use_phonemes: True\n",
      "\t| > phonemizer:\n",
      "\t\t| > phoneme language: en-us\n",
      "\t\t| > phoneme backend: gruut\n",
      "\t| > 1 not found characters:\n",
      "\t| > ͡\n",
      "| > Number of instances : 12\n",
      " | > Preprocessing samples\n",
      " | > Max text length: 908\n",
      " | > Min text length: 106\n",
      " | > Avg text length: 392.8181818181818\n",
      " | \n",
      " | > Max audio length: 4811245.0\n",
      " | > Min audio length: 645483.0\n",
      " | > Avg audio length: 2134389.1818181816\n",
      " | > Num. instances discarded samples: 1\n",
      " | > Batch group size: 0.\n",
      "Self.OutPath C:\\Users\\stonk013\\KGO_Projects\\tts\\ama_model\\tts_train_dir\\run-August-10-2023_09+29PM-0000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py\", line 1806, in fit\n",
      "    self._fit()\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py\", line 1758, in _fit\n",
      "    self.train_epoch()\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py\", line 1485, in train_epoch\n",
      "    for cur_step, batch in enumerate(self.train_loader):\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 633, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1345, in _next_data\n",
      "    return self._process_data(data)\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1371, in _process_data\n",
      "    data.reraise()\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\_utils.py\", line 644, in reraise\n",
      "    raise exception\n",
      "AssertionError: Caught AssertionError in DataLoader worker process 0.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n",
      "    data = fetcher.fetch(index)\n",
      "  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n",
      "    return self.collate_fn(data)\n",
      "  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\datasets\\dataset.py\", line 464, in collate_fn\n",
      "    mel = prepare_tensor(mel, self.outputs_per_step)\n",
      "  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\utils\\data.py\", line 29, in prepare_tensor\n",
      "    return np.stack([_pad_tensor(x, pad_len) for x in inputs])\n",
      "  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\utils\\data.py\", line 29, in <listcomp>\n",
      "    return np.stack([_pad_tensor(x, pad_len) for x in inputs])\n",
      "  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\utils\\data.py\", line 20, in _pad_tensor\n",
      "    assert x.ndim == 2\n",
      "AssertionError\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py:1806\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1806\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py:1758\u001b[0m, in \u001b[0;36mTrainer._fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_train_epoch \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_with_eval:\n\u001b[1;32m-> 1758\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrun_eval:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py:1485\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1484\u001b[0m batch_num_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n\u001b[1;32m-> 1485\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cur_step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader):\n\u001b[0;32m   1486\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(batch, batch_num_steps, cur_step, loader_start_time)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\stonk013\\Anaconda3\\envs\\tts\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\datasets\\dataset.py\", line 464, in collate_fn\n    mel = prepare_tensor(mel, self.outputs_per_step)\n  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\utils\\data.py\", line 29, in prepare_tensor\n    return np.stack([_pad_tensor(x, pad_len) for x in inputs])\n  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\utils\\data.py\", line 29, in <listcomp>\n    return np.stack([_pad_tensor(x, pad_len) for x in inputs])\n  File \"C:\\Users\\stonk013\\KGO_Projects\\tts\\TTS\\TTS\\tts\\utils\\data.py\", line 20, in _pad_tensor\n    assert x.ndim == 2\nAssertionError\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 101\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# AND... 3,2,1... 🚀\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\trainer\\trainer.py:1842\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1841\u001b[0m traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m-> 1842\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2095\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2093\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2094\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2095\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2096\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2099\u001b[0m         \u001b[38;5;66;03m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m         \u001b[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   2101\u001b[0m         \u001b[38;5;66;03m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:696\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:559\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    556\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    557\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    558\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 559\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:1396\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:1287\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1284\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:1140\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1133\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1138\u001b[0m ):\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1140\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:1030\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1028\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1029\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1033\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1034\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tts\\lib\\site-packages\\IPython\\core\\ultratb.py:1098\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1098\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from trainer import Trainer, TrainerArgs\n",
    "\n",
    "from TTS.config.shared_configs import BaseAudioConfig\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.configs.tacotron2_config import Tacotron2Config\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.models.tacotron2 import Tacotron2\n",
    "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
    "from TTS.utils.audio import AudioProcessor\n",
    "\n",
    "# from TTS.tts.datasets.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "dataset_config = BaseDatasetConfig(\n",
    "    #formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=os.path.join(os.path.join(output_path,\"wavs\"))\n",
    "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", language=\"en-us\", path=output_path\n",
    ")\n",
    "\n",
    "audio_config = BaseAudioConfig(\n",
    "    sample_rate=frame_rate,\n",
    "    do_trim_silence=True,\n",
    "    trim_db=60.0,\n",
    "    signal_norm=False,\n",
    "    mel_fmin=0.0,\n",
    "    mel_fmax=8000,\n",
    "    spec_gain=1.0,\n",
    "    log_func=\"np.log\",\n",
    "    ref_level_db=20,\n",
    "    preemphasis=0.0,\n",
    ")\n",
    "\n",
    "config = Tacotron2Config(  # This is the config that is saved for the future use\n",
    "    audio=audio_config,\n",
    "    batch_size=64,\n",
    "    eval_batch_size=16,\n",
    "    num_loader_workers=4,\n",
    "    num_eval_loader_workers=4,\n",
    "    run_eval=True,\n",
    "    test_delay_epochs=-1,\n",
    "    ga_alpha=0.0,\n",
    "    decoder_loss_alpha=0.25,\n",
    "    postnet_loss_alpha=0.25,\n",
    "    postnet_diff_spec_alpha=0,\n",
    "    decoder_diff_spec_alpha=0,\n",
    "    decoder_ssim_alpha=0,\n",
    "    postnet_ssim_alpha=0,\n",
    "    r=2,\n",
    "    attention_type=\"dynamic_convolution\",\n",
    "    double_decoder_consistency=False,\n",
    "    epochs=1000,\n",
    "    text_cleaner=\"phoneme_cleaners\",\n",
    "    use_phonemes=True,\n",
    "    phoneme_language=\"en-us\",\n",
    "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
    "    print_step=25,\n",
    "    print_eval=True,\n",
    "    mixed_precision=False,\n",
    "    output_path=output_path,\n",
    "    datasets=[dataset_config],\n",
    ")\n",
    "\n",
    "# INITIALIZE THE AUDIO PROCESSOR\n",
    "# Audio processor is used for feature extraction and audio I/O.\n",
    "# It mainly serves to the dataloader and the training loggers.\n",
    "ap = AudioProcessor.init_from_config(config)\n",
    "\n",
    "# INITIALIZE THE TOKENIZER\n",
    "# Tokenizer is used to convert text to sequences of token IDs.\n",
    "# If characters are not defined in the config, default characters are passed to the config\n",
    "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
    "\n",
    "# LOAD DATA SAMPLES\n",
    "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
    "# You can define your custom sample loader returning the list of samples.\n",
    "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
    "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    dataset_config,\n",
    "    eval_split=False,\n",
    "    eval_split_max_size=config.eval_split_max_size,\n",
    "    eval_split_size=config.eval_split_size,\n",
    ")\n",
    "\n",
    "# INITIALIZE THE MODEL\n",
    "# Models take a config object and a speaker manager as input\n",
    "# Config defines the details of the model like the number of layers, the size of the embedding, etc.\n",
    "# Speaker manager is used by multi-speaker models.\n",
    "model = Tacotron2(config, ap, tokenizer)\n",
    "\n",
    "# INITIALIZE THE TRAINER\n",
    "# Trainer provides a generic API to train all the 🐸TTS models with all its perks like mixed-precision training,\n",
    "# distributed training, etc.\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n",
    ")\n",
    "\n",
    "# AND... 3,2,1... 🚀\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e4434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b995db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
